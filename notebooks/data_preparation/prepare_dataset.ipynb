{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b510ba7-99a2-49b6-8b8c-cfbb54387ef3",
   "metadata": {},
   "source": [
    "# Converting the Drainage Crossings Dataset to COCO Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aaeb878a-5b4f-4f29-b7b9-7259230c064a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shutil\n",
    "import json\n",
    "import xml.etree.ElementTree as ET\n",
    "import rasterio\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e60fc79-9d79-403a-94da-7da7f26acfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change to dataset path, all else can then be run without modifying\n",
    "dataset_path = \"/workspace/Data_share/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03a6b7bc-0272-49cb-aec2-20ea2fa4f6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "directories = ['CA', 'IL', 'NE']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7f0f2f-8c76-41bf-8d85-38d87f0cebb0",
   "metadata": {},
   "source": [
    "## 1. Renaming files, creating a data catalog, and splitting the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4974bed-0f4c-4e6f-a4b9-a3956af13969",
   "metadata": {},
   "source": [
    "### Renaming files for consistency across sub-directories and differentiability among images - only perform these steps once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02473234-7a88-47e8-b20f-b58b4193e082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename all image files to reflect their physical location\n",
    "for d in directories:\n",
    "    dirpath = os.path.join(dataset_path, d, f'{d}_data')\n",
    "    for file in os.listdir(dirpath):\n",
    "        rename = f\"{d}_{file}\"\n",
    "        new_path = os.path.join(dirpath, rename)\n",
    "        os.rename(os.path.join(dirpath, file), new_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea602655-efeb-4d1f-b3b6-e3bc7fc4cdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename annotation directories for consistency\n",
    "os.rename(os.path.join(dataset_path, 'CA', 'annotations'), os.path.join(dataset_path, 'CA', 'CA_annotations'))\n",
    "os.rename(os.path.join(dataset_path, 'IL', 'annotations'), os.path.join(dataset_path, 'IL', 'IL_annotations'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "169f6910-053b-40fd-ad29-5775e7c80f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename all annotation files to match their corresponding image\n",
    "for d in directories:\n",
    "    dirpath = os.path.join(dataset_path, d, f'{d}_annotations')\n",
    "    for file in os.listdir(dirpath):\n",
    "        rename = f\"{d}_{file}\"\n",
    "        new_path = os.path.join(dirpath, rename)\n",
    "        os.rename(os.path.join(dirpath, file), new_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c31acea-eb04-41b4-9cff-2d9677ba9a6e",
   "metadata": {},
   "source": [
    "### Creating the data catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3589dd8c-ddf4-411f-8659-0722deea0be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe with files, annotations, and corresponding paths\n",
    "file_list = []\n",
    "for d in directories:\n",
    "    dirpath = os.path.join(dataset_path, d, f'{d}_data')\n",
    "    for file in os.listdir(dirpath):\n",
    "        if file.endswith('.tif'):\n",
    "            annotation = file[:-4]+'.xml'\n",
    "            file_list.append({\n",
    "                'filename': file, \n",
    "                'filepath':os.path.join(dirpath, file),\n",
    "                'annpath': os.path.join(dataset_path, d, f'{d}_annotations', annotation),\n",
    "                'ann': annotation\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2164fe7-3cc2-46fd-95c7-8d98f894f890",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>filepath</th>\n",
       "      <th>annpath</th>\n",
       "      <th>ann</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CA_612.tif</td>\n",
       "      <td>/workspace/Data_share/CA/CA_data/CA_612.tif</td>\n",
       "      <td>/workspace/Data_share/CA/CA_annotations/CA_612...</td>\n",
       "      <td>CA_612.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CA_341.tif</td>\n",
       "      <td>/workspace/Data_share/CA/CA_data/CA_341.tif</td>\n",
       "      <td>/workspace/Data_share/CA/CA_annotations/CA_341...</td>\n",
       "      <td>CA_341.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CA_1878.tif</td>\n",
       "      <td>/workspace/Data_share/CA/CA_data/CA_1878.tif</td>\n",
       "      <td>/workspace/Data_share/CA/CA_annotations/CA_187...</td>\n",
       "      <td>CA_1878.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CA_1504.tif</td>\n",
       "      <td>/workspace/Data_share/CA/CA_data/CA_1504.tif</td>\n",
       "      <td>/workspace/Data_share/CA/CA_annotations/CA_150...</td>\n",
       "      <td>CA_1504.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CA_2214.tif</td>\n",
       "      <td>/workspace/Data_share/CA/CA_data/CA_2214.tif</td>\n",
       "      <td>/workspace/Data_share/CA/CA_annotations/CA_221...</td>\n",
       "      <td>CA_2214.xml</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      filename                                      filepath  \\\n",
       "0   CA_612.tif   /workspace/Data_share/CA/CA_data/CA_612.tif   \n",
       "1   CA_341.tif   /workspace/Data_share/CA/CA_data/CA_341.tif   \n",
       "2  CA_1878.tif  /workspace/Data_share/CA/CA_data/CA_1878.tif   \n",
       "3  CA_1504.tif  /workspace/Data_share/CA/CA_data/CA_1504.tif   \n",
       "4  CA_2214.tif  /workspace/Data_share/CA/CA_data/CA_2214.tif   \n",
       "\n",
       "                                             annpath          ann  \n",
       "0  /workspace/Data_share/CA/CA_annotations/CA_612...   CA_612.xml  \n",
       "1  /workspace/Data_share/CA/CA_annotations/CA_341...   CA_341.xml  \n",
       "2  /workspace/Data_share/CA/CA_annotations/CA_187...  CA_1878.xml  \n",
       "3  /workspace/Data_share/CA/CA_annotations/CA_150...  CA_1504.xml  \n",
       "4  /workspace/Data_share/CA/CA_annotations/CA_221...  CA_2214.xml  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(file_list)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7e41ac-413c-4c19-81f3-27819000fcbc",
   "metadata": {},
   "source": [
    "### Ensure that image chips do not contain invalid values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4339e8a7-5655-425a-aa8c-6704fe2b98c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████████████████████████       | 4335/5400 [01:13<00:16, 63.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 3.402823e+38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|█████████████████████████████      | 4489/5400 [01:16<00:14, 63.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 3.402823e+38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|█████████████████████████████▍     | 4545/5400 [01:17<00:13, 63.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 3.402823e+38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|██████████████████████████████████▏| 5280/5400 [01:29<00:01, 63.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 3.402823e+38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 5400/5400 [01:30<00:00, 59.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 out of 5400 chips filtered out due to invalid pixel values\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df['valid'] = True\n",
    "for index, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    with rasterio.open(row['filepath'], 'r') as src:\n",
    "        data = src.read()\n",
    "    # this operation catches invalid values that are too high or too low compared to the rest of the DEM\n",
    "    df.at[index, 'valid'] = False if (np.max(data) - np.min(data)) > 10000 else True\n",
    "    if (np.max(data) - np.min(data)) > 10000:\n",
    "        print (np.min(data), np.max(data))\n",
    "\n",
    "filtered = len(df[df['valid']==False])\n",
    "\n",
    "print(f'{filtered} out of {len(df)} chips filtered out due to invalid pixel values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4dd0ab3-7665-489d-a449-a0adf06759d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['valid']==True]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c39aff-cace-428a-9382-0bdc134e4d1e",
   "metadata": {},
   "source": [
    "### Randomly split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ca03a79-ecb1-4a1d-ac38-3b22969a7728",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>filepath</th>\n",
       "      <th>annpath</th>\n",
       "      <th>ann</th>\n",
       "      <th>valid</th>\n",
       "      <th>usage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CA_612.tif</td>\n",
       "      <td>/workspace/Data_share/CA/CA_data/CA_612.tif</td>\n",
       "      <td>/workspace/Data_share/CA/CA_annotations/CA_612...</td>\n",
       "      <td>CA_612.xml</td>\n",
       "      <td>True</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CA_341.tif</td>\n",
       "      <td>/workspace/Data_share/CA/CA_data/CA_341.tif</td>\n",
       "      <td>/workspace/Data_share/CA/CA_annotations/CA_341...</td>\n",
       "      <td>CA_341.xml</td>\n",
       "      <td>True</td>\n",
       "      <td>validate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CA_1878.tif</td>\n",
       "      <td>/workspace/Data_share/CA/CA_data/CA_1878.tif</td>\n",
       "      <td>/workspace/Data_share/CA/CA_annotations/CA_187...</td>\n",
       "      <td>CA_1878.xml</td>\n",
       "      <td>True</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CA_1504.tif</td>\n",
       "      <td>/workspace/Data_share/CA/CA_data/CA_1504.tif</td>\n",
       "      <td>/workspace/Data_share/CA/CA_annotations/CA_150...</td>\n",
       "      <td>CA_1504.xml</td>\n",
       "      <td>True</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CA_2214.tif</td>\n",
       "      <td>/workspace/Data_share/CA/CA_data/CA_2214.tif</td>\n",
       "      <td>/workspace/Data_share/CA/CA_annotations/CA_221...</td>\n",
       "      <td>CA_2214.xml</td>\n",
       "      <td>True</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      filename                                      filepath  \\\n",
       "0   CA_612.tif   /workspace/Data_share/CA/CA_data/CA_612.tif   \n",
       "1   CA_341.tif   /workspace/Data_share/CA/CA_data/CA_341.tif   \n",
       "2  CA_1878.tif  /workspace/Data_share/CA/CA_data/CA_1878.tif   \n",
       "3  CA_1504.tif  /workspace/Data_share/CA/CA_data/CA_1504.tif   \n",
       "4  CA_2214.tif  /workspace/Data_share/CA/CA_data/CA_2214.tif   \n",
       "\n",
       "                                             annpath          ann  valid  \\\n",
       "0  /workspace/Data_share/CA/CA_annotations/CA_612...   CA_612.xml   True   \n",
       "1  /workspace/Data_share/CA/CA_annotations/CA_341...   CA_341.xml   True   \n",
       "2  /workspace/Data_share/CA/CA_annotations/CA_187...  CA_1878.xml   True   \n",
       "3  /workspace/Data_share/CA/CA_annotations/CA_150...  CA_1504.xml   True   \n",
       "4  /workspace/Data_share/CA/CA_annotations/CA_221...  CA_2214.xml   True   \n",
       "\n",
       "      usage  \n",
       "0     train  \n",
       "1  validate  \n",
       "2     train  \n",
       "3     train  \n",
       "4     train  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# randomly split dataset\n",
    "np.random.seed(0)\n",
    "def assign_usage(file):\n",
    "    n = np.random.rand()\n",
    "    if n < 0.7:\n",
    "        return 'train'\n",
    "    elif n < 0.9:\n",
    "        return 'validate'\n",
    "    else:\n",
    "        return 'test'\n",
    "\n",
    "df['usage'] = df.apply(assign_usage, axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7371bf80-90ba-4bb3-9d1e-c918f85f226f",
   "metadata": {},
   "source": [
    "## 2. Create new data directory to conform to COCO specifications and generate COCO annotation json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a0a65ab-d820-4256-9bbd-cec68ee49d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training, testing, and validation directories\n",
    "output_directory = '/workspace/processed_data_v7/initial_data'\n",
    "\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "os.chdir(output_directory)\n",
    "os.makedirs('train', exist_ok=True)\n",
    "os.makedirs('validate', exist_ok=True)\n",
    "os.makedirs('test', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67523592-eb95-4b65-bf55-cc68070a04e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 5396/5396 [00:07<00:00, 739.57it/s]\n"
     ]
    }
   ],
   "source": [
    "# use shutil to move files into directories based on the usage column\n",
    "for index, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    filepath = row['filepath']\n",
    "    usage = row['usage']\n",
    "    new_path = os.path.join(output_directory, usage, f'{index}.tif')\n",
    "    shutil.copy(filepath, new_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56952841-a489-47ea-9ab0-e5a835d0f1ac",
   "metadata": {},
   "source": [
    "### Iterating through annotation files to create the COCO json for each directory\n",
    "\n",
    "As validation and testing utilizes center crop, we filter bounding boxes which fall outside this center crop portion. \n",
    "This is to avoid counting these exterior bounding boxes against model performance when calling the COCO evaluator, which has no capacity to ignore objects outside the center crop zone.\n",
    "We generate train, test, and validate jsons for each cropping experiment. Training jsons will be equivalent to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8cab820f-eebb-4739-8156-00139e692b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change to annotation directory\n",
    "annotation_directory = os.path.join(output_directory, 'annotations')\n",
    "os.makedirs(annotation_directory, exist_ok=True)\n",
    "os.chdir(annotation_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "505cd230-ac0e-4d35-aa01-0ca19f36647e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_json(crop):\n",
    "    # for each given directory:\n",
    "    ann_number = 0\n",
    "    for usage in ['train', 'test', 'validate']:\n",
    "        # initialize COCO json for the directory\n",
    "        usage_json = {}\n",
    "        info = {'year': 2024,\n",
    "                'version': 1.0,\n",
    "                'description': f'Data to {usage} drainage culvert detection task',\n",
    "                'contributor': 'none',\n",
    "                'url': 'none',\n",
    "                'date_created':None}\n",
    "        licenses = [{'id':1,\n",
    "                    'url':'https://creativecommons.org/publicdomain/zero/1.0/',\n",
    "                    'name':'Public Domain'}]\n",
    "        categories = [{'id':1,\n",
    "                       'name':'Drainage Culvert',\n",
    "                       'supercategory':'none'}]\n",
    "        images = []\n",
    "        annotations = []\n",
    "        # get a dataframe of only files for a given usage\n",
    "        usage_df = df[df['usage'] == usage]\n",
    "        for index, row in tqdm(usage_df.iterrows(), total=len(usage_df)):\n",
    "            # add image information to json\n",
    "            image_dict = {'id':index,\n",
    "                          'license':1,\n",
    "                          'file_name':f'{index}.tif',\n",
    "                          'height':800,\n",
    "                          'width':800,\n",
    "                          'date_captured':'none'}\n",
    "            images.append(image_dict)\n",
    "            # load in annotation xml to dict\n",
    "            tree = ET.parse(row['annpath'])\n",
    "            root = tree.getroot()\n",
    "    \n",
    "            \n",
    "            # extract bounding box from dictionary and append to json as annotation\n",
    "            for bbox in root.findall('object'):\n",
    "                xmin = int(bbox.find('bndbox/xmin').text)\n",
    "                ymin = int(bbox.find('bndbox/ymin').text)\n",
    "                xmax = int(bbox.find('bndbox/xmax').text)\n",
    "                ymax = int(bbox.find('bndbox/ymax').text)\n",
    "    \n",
    "                limit = ((800 - crop) / 2) + 50 # sets a box outside of which to exclude centroids\n",
    "                if xmax <= limit and usage != 'train':\n",
    "                    continue\n",
    "                elif xmin >= 800 - limit and usage != 'train':\n",
    "                    continue\n",
    "                elif ymax <= limit and usage != 'train':\n",
    "                    continue\n",
    "                elif ymin >= 800 - limit and usage != 'train':\n",
    "                    continue\n",
    "                    \n",
    "                width = xmax - xmin\n",
    "                height = ymax - ymin\n",
    "                annotation_dict = {'id': ann_number,\n",
    "                              'image_id':index,\n",
    "                              'category_id': 1,\n",
    "                              'bbox': [xmin, ymin, width, height],\n",
    "                              'area': width*height,\n",
    "                              'segmentation':[],\n",
    "                              'iscrowd':0}\n",
    "                annotations.append(annotation_dict)\n",
    "                ann_number += 1\n",
    "        usage_json['info'] = info\n",
    "        usage_json['licenses'] = licenses\n",
    "        usage_json['categories'] = categories\n",
    "        usage_json['images'] = images\n",
    "        usage_json['annotations'] = annotations\n",
    "        \n",
    "        json_data = json.dumps(usage_json, indent=4)\n",
    "    \n",
    "            \n",
    "        with open(f'{usage}_{crop}.json', 'w') as json_file:\n",
    "            json_file.write(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "36ca56d2-f30c-4b8b-b558-010c1e620dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 3800/3800 [00:00<00:00, 7013.45it/s]\n",
      "100%|███████████████████████████████████| 547/547 [00:00<00:00, 7939.03it/s]\n",
      "100%|█████████████████████████████████| 1049/1049 [00:00<00:00, 7937.97it/s]\n",
      "100%|█████████████████████████████████| 3800/3800 [00:00<00:00, 7133.23it/s]\n",
      "100%|███████████████████████████████████| 547/547 [00:00<00:00, 7935.43it/s]\n",
      "100%|█████████████████████████████████| 1049/1049 [00:00<00:00, 7899.53it/s]\n",
      "100%|█████████████████████████████████| 3800/3800 [00:00<00:00, 7913.95it/s]\n",
      "100%|███████████████████████████████████| 547/547 [00:00<00:00, 7814.16it/s]\n",
      "100%|█████████████████████████████████| 1049/1049 [00:00<00:00, 7772.43it/s]\n",
      "100%|█████████████████████████████████| 3800/3800 [00:00<00:00, 7844.75it/s]\n",
      "100%|███████████████████████████████████| 547/547 [00:00<00:00, 4796.63it/s]\n",
      "100%|█████████████████████████████████| 1049/1049 [00:00<00:00, 7721.62it/s]\n"
     ]
    }
   ],
   "source": [
    "for crop in [256, 400, 600, 800]:\n",
    "    generate_json(crop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ba7e53-262e-412b-b1cd-9a7552f5c8ac",
   "metadata": {},
   "source": [
    "## Perform steps to create transfer dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bf7a527-fadf-482a-9a67-239573d5cb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "directories = ['ND']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a0c015-ea89-487b-a417-a030d6827102",
   "metadata": {},
   "source": [
    "### Rename files, following previous conventions - only run this subsection once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91017d01-bdf7-4dea-88d2-63027e60345a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename all image files to reflect their physical location\n",
    "for d in directories:\n",
    "    dirpath = os.path.join(dataset_path, d, f'{d}_data')\n",
    "    for file in os.listdir(dirpath):\n",
    "        rename = f\"{d}_{file}\"\n",
    "        new_path = os.path.join(dirpath, rename)\n",
    "        os.rename(os.path.join(dirpath, file), new_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7b98410-0db3-4219-91b5-2e759eda5d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename all annotation files to match their corresponding image\n",
    "for d in directories:\n",
    "    dirpath = os.path.join(dataset_path, d, f'{d}_annotations')\n",
    "    for file in os.listdir(dirpath):\n",
    "        rename = f\"{d}_{file}\"\n",
    "        new_path = os.path.join(dirpath, rename)\n",
    "        os.rename(os.path.join(dirpath, file), new_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd4fb33-1ef0-4b8c-a08b-81d77851db72",
   "metadata": {},
   "source": [
    "### Creating the transfer learning dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c1e3420-8e01-4301-bf46-88f4fee21621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe with files, annotations, and corresponding paths\n",
    "file_list = []\n",
    "for d in directories:\n",
    "    dirpath = os.path.join(dataset_path, d, f'{d}_data')\n",
    "    for file in os.listdir(dirpath):\n",
    "        if file.endswith('.tif'):\n",
    "            annotation = file[:-4]+'.xml'\n",
    "            file_list.append({\n",
    "                'filename': file, \n",
    "                'filepath':os.path.join(dirpath, file),\n",
    "                'annpath': os.path.join(dataset_path, d, f'{d}_annotations', annotation),\n",
    "                'ann': annotation\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b36bfbc9-3281-4861-8e32-be5cc0e65f48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>filepath</th>\n",
       "      <th>annpath</th>\n",
       "      <th>ann</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ND_299.tif</td>\n",
       "      <td>/workspace/Data_share/ND/ND_data/ND_299.tif</td>\n",
       "      <td>/workspace/Data_share/ND/ND_annotations/ND_299...</td>\n",
       "      <td>ND_299.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ND_227.tif</td>\n",
       "      <td>/workspace/Data_share/ND/ND_data/ND_227.tif</td>\n",
       "      <td>/workspace/Data_share/ND/ND_annotations/ND_227...</td>\n",
       "      <td>ND_227.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ND_457.tif</td>\n",
       "      <td>/workspace/Data_share/ND/ND_data/ND_457.tif</td>\n",
       "      <td>/workspace/Data_share/ND/ND_annotations/ND_457...</td>\n",
       "      <td>ND_457.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ND_505.tif</td>\n",
       "      <td>/workspace/Data_share/ND/ND_data/ND_505.tif</td>\n",
       "      <td>/workspace/Data_share/ND/ND_annotations/ND_505...</td>\n",
       "      <td>ND_505.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ND_516.tif</td>\n",
       "      <td>/workspace/Data_share/ND/ND_data/ND_516.tif</td>\n",
       "      <td>/workspace/Data_share/ND/ND_annotations/ND_516...</td>\n",
       "      <td>ND_516.xml</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     filename                                     filepath  \\\n",
       "0  ND_299.tif  /workspace/Data_share/ND/ND_data/ND_299.tif   \n",
       "1  ND_227.tif  /workspace/Data_share/ND/ND_data/ND_227.tif   \n",
       "2  ND_457.tif  /workspace/Data_share/ND/ND_data/ND_457.tif   \n",
       "3  ND_505.tif  /workspace/Data_share/ND/ND_data/ND_505.tif   \n",
       "4  ND_516.tif  /workspace/Data_share/ND/ND_data/ND_516.tif   \n",
       "\n",
       "                                             annpath         ann  \n",
       "0  /workspace/Data_share/ND/ND_annotations/ND_299...  ND_299.xml  \n",
       "1  /workspace/Data_share/ND/ND_annotations/ND_227...  ND_227.xml  \n",
       "2  /workspace/Data_share/ND/ND_annotations/ND_457...  ND_457.xml  \n",
       "3  /workspace/Data_share/ND/ND_annotations/ND_505...  ND_505.xml  \n",
       "4  /workspace/Data_share/ND/ND_annotations/ND_516...  ND_516.xml  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(file_list)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ce1038-14f5-4720-9285-c807ee14ff56",
   "metadata": {},
   "source": [
    "###Ensure that image chips do not contain invalid values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "808e78f8-b97a-4db1-80b1-a4e63e6c28c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██████████▊                          | 179/612 [00:03<00:07, 60.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 3.402823e+38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████████████████▍                    | 272/612 [00:04<00:05, 57.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 3.402823e+38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████▎                  | 303/612 [00:05<00:05, 58.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 3.402823e+38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|███████████████████▊                 | 328/612 [00:05<00:04, 59.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 3.402823e+38\n",
      "0.0 3.402823e+38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|███████████████████████▉             | 395/612 [00:06<00:03, 58.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 3.402823e+38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|████████████████████████████▍        | 470/612 [00:08<00:02, 60.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 3.402823e+38\n",
      "0.0 3.402823e+38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 612/612 [00:10<00:00, 57.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 out of 612 chips filtered out due to invalid pixel values\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df['valid'] = True\n",
    "for index, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    with rasterio.open(row['filepath'], 'r') as src:\n",
    "        data = src.read()\n",
    "    # this operation catches invalid values that are too high or too low compared to the rest of the DEM\n",
    "    df.at[index, 'valid'] = False if (np.max(data) - np.min(data)) > 10000 else True\n",
    "    if (np.max(data) - np.min(data)) > 10000:\n",
    "        print (np.min(data), np.max(data))\n",
    "\n",
    "filtered = len(df[df['valid']==False])\n",
    "\n",
    "print(f'{filtered} out of {len(df)} chips filtered out due to invalid pixel values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eee5b038-723f-4d19-9d85-9977ba1177ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['valid']==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e522a6a8-5966-445d-9124-8abc010e08ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create annotation directory\n",
    "output_directory = '/workspace/processed_data_v7/transfer_data'\n",
    "\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "os.chdir(output_directory)\n",
    "os.makedirs('test', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30b2cfbd-626a-4db2-b24a-2c99fa6526f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 604/604 [00:00<00:00, 639.12it/s]\n"
     ]
    }
   ],
   "source": [
    "# as we are using this dataset entirely for testing, random splitting is not necessary\n",
    "\n",
    "df['usage'] = 'test'\n",
    "# use shutil to move files into directories based on the usage column\n",
    "for index, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    filepath = row['filepath']\n",
    "    usage = row['usage']\n",
    "    new_path = os.path.join(output_directory, usage, f'{index}.tif')\n",
    "    shutil.copy(filepath, new_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d060d9b-313b-49d8-b45e-50371f08555d",
   "metadata": {},
   "source": [
    "## Iterating through annotation files to create the COCO json for each directory\n",
    "\n",
    "Once again, we filter bounding boxes which fall outside the zone of the image that will be cropped to in testing for each crop we are utilizing in experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58322a1f-552a-4d46-8f6f-5482663b0981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change to annotation directory\n",
    "annotation_directory = os.path.join(output_directory, 'annotations')\n",
    "os.makedirs(annotation_directory, exist_ok=True)\n",
    "os.chdir(annotation_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c788d3b8-6451-484c-b9cb-60f5b1a6477b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_json(crop):\n",
    "    # for each given directory:\n",
    "    ann_number = 0\n",
    "    for usage in ['test']:\n",
    "        # initialize COCO json for the directory\n",
    "        usage_json = {}\n",
    "        info = {'year': 2024,\n",
    "                'version': 1.0,\n",
    "                'description': f'Data to {usage} drainage culvert detection task',\n",
    "                'contributor': 'none',\n",
    "                'url': 'none',\n",
    "                'date_created':None}\n",
    "        licenses = [{'id':1,\n",
    "                    'url':'https://creativecommons.org/publicdomain/zero/1.0/',\n",
    "                    'name':'Public Domain'}]\n",
    "        categories = [{'id':1,\n",
    "                       'name':'Drainage Culvert',\n",
    "                       'supercategory':'none'}]\n",
    "        images = []\n",
    "        annotations = []\n",
    "        # get a dataframe of only files for a given usage\n",
    "        usage_df = df[df['usage'] == usage]\n",
    "        for index, row in tqdm(usage_df.iterrows(), total=len(usage_df)):\n",
    "            # add image information to json\n",
    "            image_dict = {'id':index,\n",
    "                          'license':1,\n",
    "                          'file_name':f'{index}.tif',\n",
    "                          'height':800,\n",
    "                          'width':800,\n",
    "                          'date_captured':'none'}\n",
    "            images.append(image_dict)\n",
    "            # load in annotation xml to dict\n",
    "            tree = ET.parse(row['annpath'])\n",
    "            root = tree.getroot()\n",
    "    \n",
    "            \n",
    "            # extract bounding box from dictionary and append to json as annotation\n",
    "            for bbox in root.findall('object'):\n",
    "                xmin = int(bbox.find('bndbox/xmin').text)\n",
    "                ymin = int(bbox.find('bndbox/ymin').text)\n",
    "                xmax = int(bbox.find('bndbox/xmax').text)\n",
    "                ymax = int(bbox.find('bndbox/ymax').text)\n",
    "    \n",
    "                limit = ((800 - crop) / 2) + 50 # sets a box outside of which to exclude centroids\n",
    "                if xmax <= limit and usage != 'train':\n",
    "                    continue\n",
    "                elif xmin >= 800 - limit and usage != 'train':\n",
    "                    continue\n",
    "                elif ymax <= limit and usage != 'train':\n",
    "                    continue\n",
    "                elif ymin >= 800 - limit and usage != 'train':\n",
    "                    continue\n",
    "                    \n",
    "                width = xmax - xmin\n",
    "                height = ymax - ymin\n",
    "                annotation_dict = {'id': ann_number,\n",
    "                              'image_id':index,\n",
    "                              'category_id': 1,\n",
    "                              'bbox': [xmin, ymin, width, height],\n",
    "                              'area': width*height,\n",
    "                              'segmentation':[],\n",
    "                              'iscrowd':0}\n",
    "                annotations.append(annotation_dict)\n",
    "                ann_number += 1\n",
    "        usage_json['info'] = info\n",
    "        usage_json['licenses'] = licenses\n",
    "        usage_json['categories'] = categories\n",
    "        usage_json['images'] = images\n",
    "        usage_json['annotations'] = annotations\n",
    "        \n",
    "        json_data = json.dumps(usage_json, indent=4)\n",
    "    \n",
    "            \n",
    "        with open(f'{usage}_{crop}.json', 'w') as json_file:\n",
    "            json_file.write(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "af65fbe7-f20c-4060-9aae-17a923a622c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 604/604 [00:00<00:00, 5143.66it/s]\n",
      "100%|███████████████████████████████████| 604/604 [00:00<00:00, 8687.94it/s]\n",
      "100%|███████████████████████████████████| 604/604 [00:00<00:00, 9114.28it/s]\n",
      "100%|███████████████████████████████████| 604/604 [00:00<00:00, 9034.23it/s]\n"
     ]
    }
   ],
   "source": [
    "for crop in [256, 400, 600, 800]:\n",
    "    generate_json(crop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e7cba9-a604-420b-98db-5939628a27f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
